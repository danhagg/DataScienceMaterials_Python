{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/Output\n",
    "###### Input\n",
    "```python\n",
    "# For urls to work I had to paste into shell: /Applications/Python\\ 3.6/Install\\ Certificates.command\n",
    "pd.read_csv(filename/url=) # From a CSV file .dropna(how=\"all)\n",
    "pd.read_table(filename) # From a delimited text file (like TSV)\n",
    "pd.read_excel(filename) # From an Excel file\n",
    "pd.read_sql(query, connection_object) # Read from a SQL table/database\n",
    "pd.read_json(json_string) # Read from a JSON formatted string, URL or file.\n",
    "pd.read_html(url) # Parses an html URL, string or file and extracts tables to a list of dataframes\n",
    "pd.read_clipboard() # Takes the contents of your clipboard and passes it to read_table()\n",
    "pd.DataFrame(dict) # From a dict, keys for columns names, values for data as lists\n",
    "```\n",
    "\n",
    "###### Output to excel... 3 steps\n",
    "```python\n",
    "excel_file = pd.ExcelWriter(path, engine=None, **kwargs) # Class for writing DataFrame objects into excel sheets, default is to use xlwt for xls, openpyxl for xlsx.  See DataFrame.to_excel for typical usage.\n",
    "df.to_excel(excel_file, sheet_name='df', index=False, columns=['col1', 'col2']) # Write to an Excel file\n",
    "excel_file.save()\n",
    "```\n",
    "###### Output to SQL\n",
    "```python\n",
    "df.to_sql(table_name, connection_object) # Write to a SQL table\n",
    "```\n",
    "\n",
    "###### Other Output\n",
    "```python\n",
    "df.to_csv(filename, index=, columns=, encoding='utf-8') # Write to a CSV file\n",
    "df.to_json(filename) # Write to a file in JSON format\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing/Inspecting Data\n",
    "```python\n",
    "df.info() # Index, Datatype and Memory information\n",
    "df.describe() # Summary statistics for numerical columns\n",
    "df.head(n) # First n rows of the DataFrame\n",
    "df.tail(n) # Last n rows of the DataFrame\n",
    "df.shape() # Number of rows and columns\n",
    "s.value_counts(dropna=False) # View unique values and counts\n",
    "df.apply(pd.Series.value_counts) # Unique values and counts for all columns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "###### Indexes/Columns\n",
    "```python\n",
    "df.set_index('column_one') # Change the index\n",
    "df.sort_index(inplace=True) # sorting the index allows panda to access/index faster with future methods. \n",
    "df.sort_index(ascending=[True, False]) # Sorting can be done with multiIndex\n",
    "df.reset_index() # Put index back to a col, and numerate the index as original\n",
    "df.set_index(keys=['col1','col2']) # Makes a multi-index with col1-col2, least-most indexes\n",
    "df.index.get_level_values(level) # Return vector of label values for requested level, equal to the length of the index\n",
    "df.index.set_names(['index1_name', 'index2_name']) # set new names for indices1/2\n",
    "df.columns/index = ['a','b','c'] # Rename columns/index\n",
    "df.columns/index = [i.replace(\" \", \"_\") for i in df.columns] # Replace column/index heading spaces \" \", with underscores\"_\"\n",
    "df.rename(columns/index=lambda x: x + 1) # Mass renaming of columns/index\n",
    "df.rename(columns={'old_name': 'new_ name'}) # Selective renaming\n",
    "df.swaplevel(i=-2, j=-1, axis=0) # Swap levels i and j in a MultiIndex on a particular axis\n",
    "df.stack().to_frame() # puts cols into index giving a 1d series. Can be converted to a df with to_frame(). Level can be index num, name or list.\n",
    "df.unstack() # puts index into cols\n",
    "df.unstack(level=-1, fill_value=None)# Pivot a level of the (necessarily hierarchical) index labels, returning a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex). The level involved will automatically get sorted.\n",
    "df.pivot(index=None, columns=None, values=None) # Reshape data (produce a \"pivot\" table) based on column values. Uses unique values from index / columns to form axes of the resulting DataFrame.\n",
    "df.pivot_table(values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All') # Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.\n",
    "df.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None) # \"Unpivots\" a DataFrame from wide format to long format, optionally leaving identifier variables set.\n",
    "```\n",
    "\n",
    "###### Strings\n",
    "```python\n",
    "df.s = df.s.str.replace(\"x\", \"y\") # Replace \"x\" with \"y\" in all cells of series s\n",
    "df.s.str.lower().str.contains(\"x\") # returns a Boolean for all cells in s containing lower case x\n",
    "df.s.str.lower().str.startswith(\"x\") # returns a Boolean for all cells in s cstarting with x (endswith)\n",
    "df.s.str.lstrip() # removes whitespace from left of string\n",
    "df.s.str.split(\",\").str.get(0).str.title() # returns first item as title for all cells in a list split on the comma\n",
    "df.s.str.split(\" \", expand=True, n=3) # makes 3 cols on a table of s\n",
    "df['col_name'] = df['col_name'].str.title() # puts all values of a col into title, note str prefix\n",
    "df['str_lengths'] = df['col_name'].str.len() # puts all lengths into new col, note str prefix\n",
    "```\n",
    "\n",
    "###### na/nulls\n",
    "```python\n",
    "pd.isnull() # Checks for null Values, Returns Boolean Arrray\n",
    "pd.notnull() # Opposite of pd.isnull()\n",
    "df.dropna(axis=1) # Drop all columns that contain null values\n",
    "df.dropna(axis=1,thresh=n) # Drop all rows have have less than n non null values\n",
    "df.drop(labels, axis=0, level=None, inplace=False, errors='raise') #Return new object with labels in requested axis removed.\n",
    "df.fillna(x) # Replace all null values with x\n",
    "s.fillna(s.mean()) # Replace all null values with the mean (mean can be replaced with almost any function from the statistics section)\n",
    "```\n",
    "\n",
    "###### Replacing data\n",
    "```python\n",
    "s.astype('float', 'bool', 'category') # Convert the datatype of the series \n",
    "s.replace(1,'one') # Replace all values equal to 1 with 'one'\n",
    "s.replace([1,3],['one','three']) # Replace all 1 with 'one' and 3 with 'three'\n",
    "del df.['col']\n",
    "df.pop('col') # standard popping, can be assign to a variable\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates\n",
    "```python\n",
    "# if Start Date is a string object like '01/31/2018'\n",
    "df['Start Date'] = pd.to_datetime(df['Start Date'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Selection\n",
    "###### Indexing\n",
    "```python\n",
    "df[col] # Returns column with label col as Series\n",
    "df[[col1, col2]] # Returns columns as a new DataFrame\n",
    "s.iloc[0] # Selection by position\n",
    "df.loc[('index1', 'index2'), 'col1'] # select rows based upon multi-indexing\n",
    "df.loc['index_one', ['col1','col2']] # Selection by index and extract col vals for that index\n",
    "df.loc[boolean_series, \"col1\"] = \"New value\"  # Change value for specific values in a series/df\n",
    "df.iloc[0,:] # First row\n",
    "df.iloc[0,0] # First element of first column\n",
    "```\n",
    "\n",
    "###### reshaping\n",
    "```python\n",
    "df.pivot_table(index=col1,values=[col2,col3],aggfunc=mean) # Create a pivot table that groups by col1 and calculates the mean of col2 and col3\n",
    "df.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) # Returns a random sample of items from an axis of object.\n",
    "df.transpose(*args, **kwargs) # Transpose index and columns\n",
    "\", \".join(str(name) for name in s[\"col_name\"]) # makes a continuous, comma-space separated string\n",
    "```\n",
    "\n",
    "###### groupby and agg\n",
    "```python\n",
    "df.groupby(col) # Returns a groupby object for values from one column\n",
    "df.groupby([col1,col2]) # Returns groupby object for values from multiple columns\n",
    "df.groupby(col1)[col2] # Returns the mean of the values in col2, grouped by the values in col1 (mean can be replaced with almost any function from the statistics section)\n",
    "groupby_object.get_group(('col1_val', 'col2_val')) # show multi_indexed group\n",
    "groupby_object.agg({'col1': \"mean\", \"col2\": \"mean\"}) # dict for each column and agg func\n",
    "groupby_object.agg([\"size\", \"sum\", \"mean\"]) # performs list funcs on all cols\n",
    "df.groupby(col1).agg(np.mean) # Find the average across all columns for every unique col1 group\n",
    "\n",
    "# iterate over groups and output top group with largest values in col_name to new_df\n",
    "new_df = pd.DataFrame(columns=df.columns) # gives same col headings as original df\n",
    "for group, data in group_obj:\n",
    "    largest = data.nlargest(1, \"col_name\")\n",
    "    new_df = new_df.append(largest)\n",
    "new_df \n",
    "```\n",
    "\n",
    "###### selections based upon data\n",
    "```python\n",
    "df[df[col] > 0.5] # Rows where the column col is greater than 0.5\n",
    "df[(df[col] > 0.5) & (df[col] < 0.7)] # Rows where 0.7 > col > 0.5\n",
    "df[col1].isin([col1-v1, col1-v2, col1-v3]) # Boolean series with multiple values\n",
    "df.sort_values(col2,ascending=False) # Sort values by col2 in descending order\n",
    "df.sort_values([col1,col2],ascending=[True,False]) # Sort values by col1 in ascending order then col2 in descending order\n",
    "df[col1].between(val1, val2) # between returns a boolean series\n",
    "df.duplicated(subset=None, keep='first') # Return boolean Series denoting duplicate rows, keep can be True/False\n",
    "df.drop_duplicates(subset=None, keep='first', inplace=False) # Return df with duplicate rows removed, optionally only considering certain columns\n",
    "df.s.unique() # return list of unique values\n",
    "df.s.nunique() # return number of unique values, drops na by default\n",
    "df.s2 = df.s1.rank(ascending=False).astype(\"int\") # insert a col s2 based upon s1 ranks\n",
    "df.apply(np.mean) # Apply the function np.mean() across each column\n",
    "nf.apply(np.max,axis=1) # Apply the function np.max() across each row\n",
    "df.nlargest(n, columns, keep='first') # Get the rows of a DataFrame sorted by the `n` largest values of `columns`.\n",
    "df.nsmallest(n, columns, keep='first') # Get the rows of a DataFrame sorted by the `n` smallest values of `columns`.\n",
    "df.where(cond/bool_ser, other=nan, inplace=False, axis=None, level=None, try_cast=False, raise_on_error=True) # Return an object of same shape as self and whose corresponding entries are from self where cond is True and otherwise are from other.\n",
    "df.query(expr, inplace=False, **kwargs) # Query the columns of a frame with a boolean (plus 'and'/'or') expression. Requires column names  free of spaces\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge/Combine/Concat\n",
    "```python\n",
    "df.copy(deep=True) # Make a copy of this objects data. Good for isolating and working on series of a df.\n",
    "df1.append(df2) # Add the rows in df1 to the end of df2 (columns should be identical)\n",
    "pd.concat([df1, df2],axis=1) # Add the columns in df1 to the end of df2 (rows should be identical)\n",
    "df1.join(df2,on=col1,how='inner') # SQL-style join the columns in df1 with the columns on df2 where the rows for col have identical values. how can be one of 'left', 'right', 'outer', 'inner'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panels\n",
    "3d dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data distributions & descriptors\n",
    "```python\n",
    "df.mean() # Returns the mean of all columns\n",
    "df.corr() # Returns the correlation between columns in a DataFrame\n",
    "df.count() # Returns the number of non-null values in each DataFrame column\n",
    "df.max() # Returns the highest value in each column\n",
    "df.min() # Returns the lowest value in each column\n",
    "df.median() # Returns the median of each column\n",
    "df.std() # Returns the standard deviation of each column\n",
    "s.nunique() # number un\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scipy.stats functions\n",
    "```python\n",
    "describe(a[, axis, ddof, bias, nan_policy])\t# Compute several descriptive statistics of the passed array.\n",
    "kurtosis(a[, axis, fisher, bias, nan_policy]) # Compute the kurtosis (Fisher or Pearson) of a dataset.\n",
    "kurtosistest(a[, axis, nan_policy])\t# Test whether a dataset has normal kurtosis.\n",
    "mode(a[, axis, nan_policy])\t# Return an array of the modal (most common) value in the passed array.\n",
    "normaltest(a[, axis, nan_policy]) # Test whether a sample differs from a normal distribution.\n",
    "skew(a[, axis, bias, nan_policy]) # Compute the skewness of a data set.\n",
    "skewtest(a[, axis, nan_policy]) # Test whether the skew is different from the normal distribution.\n",
    "variation(a[, axis, nan_policy]) # Compute the coefficient of variation, the ratio of the biased standard deviation to the mean.\n",
    "cumfreq(a[, numbins, defaultreallimits, weights]) # Return a cumulative frequency histogram, using the histogram function.\n",
    "itemfreq(a) # Return a 2-D array of item frequencies.\n",
    "relfreq(a[, numbins, defaultreallimits, weights]) # Return a relative frequency histogram, using the histogram function.\n",
    "binned_statistic(x, values[, statistic, ...]) # Compute a binned statistic for one or more sets of data.\n",
    "sem(a[, axis, ddof, nan_policy]) # Calculate the standard error of the mean (or standard error of measurement) of the values in the input array.\n",
    "zmap(scores, compare[, axis, ddof]) # Calculate the relative z-scores.\n",
    "zscore(a[, axis, ddof]) # Calculate the z score of each value in the sample, relative to the sample mean and standard deviation.\n",
    "iqr(x[, axis, rng, scale, nan_policy, ...]) # Compute the interquartile range of the data along the specified axis.\n",
    "f_oneway(*args)\t# Performs a 1-way ANOVA.\n",
    "pearsonr(x, y) # Calculate a Pearson correlation coefficient and the p-value for testing non-correlation.\n",
    "spearmanr(a[, b, axis, nan_policy]) # Calculate a Spearman rank-order correlation coefficient and the p-value to test for non-correlation.\n",
    "linregress(x[, y]) # Calculate a linear least-squares regression for two sets of measurements.\n",
    "ttest_ind(a, b[, axis, equal_var, nan_policy]) # Calculate the T-test for the means of two independent samples of scores.\n",
    "ttest_rel(a, b[, axis, nan_policy]) # Calculate the T-test on TWO RELATED samples of scores, a and b.\n",
    "kstest(rvs, cdf[, args, N, alternative, mode]) # Perform the Kolmogorov-Smirnov test for goodness of fit.\n",
    "chisquare(f_obs[, f_exp, ddof, axis]) # Calculate a one-way chi square test.\n",
    "power_divergence(f_obs[, f_exp, ddof, axis, ...]) # Cressie-Read power divergence statistic and goodness of fit test.\n",
    "ks_2samp(data1, data2) # Compute the Kolmogorov-Smirnov statistic on 2 samples.\n",
    "mannwhitneyu(x, y[, use_continuity, alternative]) # Compute the Mann-Whitney rank test on samples x and y.\n",
    "tiecorrect(rankvals) # Tie correction factor for ties in the Mann-Whitney U and Kruskal-Wallis H tests.\n",
    "rankdata(a[, method]) # Assign ranks to data, dealing with ties appropriately.\n",
    "ranksums(x, y) # Compute the Wilcoxon rank-sum statistic for two samples.\n",
    "wilcoxon(x[, y, zero_method, correction]) # Calculate the Wilcoxon signed-rank test.\n",
    "kruskal(*args, \\**kwargs) # Compute the Kruskal-Wallis H-test for independent samples\n",
    "friedmanchisquare(*args) # Compute the Friedman test for repeated measurements\n",
    "shapiro(x) # Perform the Shapiro-Wilk test for normality.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
