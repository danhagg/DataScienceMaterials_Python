{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- Assume the data is clean and feature/target are in columns of a pandas dataframe\n",
    "\n",
    "##### View/prep the data\n",
    "```python\n",
    "sns.pairplot(df) # view the features and target data\n",
    "sns.distplot([df['target']]) # view target distribution\n",
    "df.corr() # make a correlation table\n",
    "sns.heatmap(df.corr(), annot=True) # make a correlation heatmap\n",
    "```\n",
    "\n",
    "#####  Assign, split and fit\n",
    "```python\n",
    "X = df[['feature_0', 'feature_1']]\n",
    "y = df['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101) # Tuple unpacking\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression() # Instantiate the object\n",
    "lm.fit(X_train, y_train) # fit the model\n",
    "```\n",
    "\n",
    "##### Coefficients, Predictions and Residuals\n",
    "```python\n",
    "cdf = pd.DataFrame(lm.coef_, X.columns, columns=['Coeff']) # make coefficients\n",
    "predictions = lm.predict(X_test) # make X_test predictions from X_train/y_train data\n",
    "sns.jointplot(y_test, predictions) # plot predictions vs actual y_test values\n",
    "sns.distplot((y_test-predictions)) # Histogram of residuals\n",
    "```\n",
    "\n",
    "##### Evaluation Metrics\n",
    "```python\n",
    "from sklearn import metrics\n",
    "metrics.mean_absolute_error(y_test, predictions) # Mean Average Error\n",
    "metrics.mean_squared_error(y_test, predictions) # Mean Squared Error\n",
    "np.sqrt(metrics.mean_absolute_error(y_test, predictions)) # Root Mean Squared Error\n",
    "metrics.explained_variance_score(y_test, predictions) # R^2. How much variance the model explains\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "- A method for classification, 0 to 1\n",
    "- Non-continuous, discrete categories, eg., binary groups\n",
    "\n",
    "### $\\theta(z) = \\frac{1}{1 + exp(-z)}$\n",
    "\n",
    "Logistic slope\n",
    "\n",
    "### $p = \\frac{1}{1 + e^{(b_0 + b_1x)}}$\n",
    "\n",
    "##### View/prep the data\n",
    "```python\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis') # view all missing data\n",
    "sns.distplot([df['target']]) # view target distribution\n",
    "\n",
    "# Impute some data\n",
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "        \n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else:\n",
    "            return 24\n",
    "            \n",
    "    else:\n",
    "        return Age\n",
    "        \n",
    "train['Age'] = train[['Age', 'Pclass']].apply(impute_age, axis=1) # Fill in age\n",
    "\n",
    "sex = pd.get_dummies(train['Sex'], drop_first=True)\n",
    "embark = pd.get_dummies(train['Embarked'], drop_first=True)\n",
    "train = pd.concat([train, sex, embark], axis=1)\n",
    "\n",
    "train.drop(['PassengerId','Sex', 'Embarked', 'Name', 'Ticket'], axis=1, inplace=True) # Drop all numerical\n",
    "```\n",
    "\n",
    "##### Assign, split and fit\n",
    "```python\n",
    "# assign\n",
    "X = train.drop('Survived', axis=1)\n",
    "y = train['Survived']\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "# fit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "##### Predictions and Evaluations\n",
    "```python\n",
    "# Predictions\n",
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest neighbours\n",
    "\n",
    "##### Scale data\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler object to feature columns only\n",
    "scaler.fit(c_data.drop('TARGET CLASS', axis=1)) \n",
    "\n",
    "# Use scalar object to do a transformation\n",
    "scaled_features = scaler.transform(c_data.drop('TARGET CLASS', axis=1))\n",
    "\n",
    "# Convert transformation array to a pandas dataframe\n",
    "df_feat = pd.DataFrame(scaled_features, columns=c_data.columns[:-1])\n",
    "```\n",
    "##### Assign, split, fit - First run through\n",
    "```python\n",
    "# Assign\n",
    "X = df_feat\n",
    "y = c_data['TARGET CLASS']\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# fit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "error_rate = []\n",
    "for i in range(1,40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "```\n",
    "\n",
    "##### Plot\n",
    "```python\n",
    "fig, ax =plt.subplots(figsize=(10,6))\n",
    "ax.plot(range(1,40), \n",
    "         error_rate, \n",
    "         color='blue', \n",
    "         linestyle='dashed', \n",
    "         marker='o',\n",
    "        markerfacecolor='red',\n",
    "        markersize=10,)\n",
    "```\n",
    "\n",
    "##### refit with best K\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Find best k from plot\n",
    "knn = KNeighborsClassifier(n_neighbors=17)\n",
    "knn.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "##### Predictions and Evaluations\n",
    "```python\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = knn.predict(X_test)\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "https://www.youtube.com/watch?v=LDRbO9a6XPU&t=382s\n",
    "\n",
    "1. Quantify how much a question serves to unimix the labels \n",
    "2. Gini impurity = Amount of uncertainty at a node\n",
    "3. We can quantify how much a question reduces Gini uncertainty by measuring information gain\n",
    "4. We can use information gain to dictate which question to ask at each node\n",
    "5. Thus we need to know \n",
    "    - which questions to ask\n",
    "    - when to ask them\n",
    "\n",
    "Each node takes a list of rows and iterates over every value of every feature. Each feature-value can be used as a threshold to partition the data in the form of a question.\n",
    "\n",
    "The best question is the one that reduces our uncertainty the most. Gini impurity (0 to 1)quantifies the uncertainty at a node. Information gain quantifies how much a question reduces that.\n",
    "\n",
    "Gini impurity. The chance of being INCORRECT if you randomly assign a label to an example in the same set.\n",
    "\n",
    "### $\\mathit{Gini}(E) = 1 - \\sum_{j=1}^{c}p_j^2$\n",
    "\n",
    "### $ = 1 - p_j$\n",
    "\n",
    "Which questions to ask?\n",
    "1. Find the Gini impurity of current node\n",
    "2. Look at dataset in that node, choose a question (we will iterate over all questions)\n",
    "3. Partition child nodes for that question. Calculate the weighted average of the uncertainty in those child nodes.\n",
    "4. Subtract this uncertainty from our starting uncertainty to yield the information gain.\n",
    "5. Iterate to the next question in the list and perform the same methods.\n",
    "6. We choose to apply the question that yields that largest information gain.\n",
    "\n",
    "##### Assign, split, fit\n",
    "```python\n",
    "# Assign\n",
    "X = data.drop('Kyphosis', axis=1)\n",
    "y = data['Kyphosis']\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# fit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "##### Predictions and Evaluations\n",
    "```python\n",
    "# Predictions\n",
    "predictions = dtree.predict(X_test)\n",
    "\n",
    "# Evaluations\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "```\n",
    "\n",
    "##### Visualisation\n",
    "```python\n",
    "from IPython.display import Image\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "features = list(data.columns[1:])\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(dtree, out_file=dot_data, feature_names=features,filled=True, rounded=True)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph[0].create_png())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "##### Assign, split, fit\n",
    "```python\n",
    "# Assign\n",
    "X = data.drop('Kyphosis', axis=1)\n",
    "y = data['Kyphosis']\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "##### Predictions and Evaluations\n",
    "```python\n",
    "rfc_predictions = rfc.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, rfc_predictions))\n",
    "print(classification_report(y_test, rfc_predictions))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means clustering\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data & Spark\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets & Deep Learning\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hippocampus Bayes\n",
    "\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
