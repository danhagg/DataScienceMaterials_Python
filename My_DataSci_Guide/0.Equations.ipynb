{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gaussian\n",
    "2. Variance\n",
    "3. STD\n",
    "4. Shapiro-Wilk\n",
    "5. Z-score\n",
    "6. Paired-t-test\n",
    "7. F-test\n",
    "8. Students ttest\n",
    "9. Welch's ttest\n",
    "10. Wilcoxon Signed Rank Test\n",
    "11. Mann Whitney U\n",
    "12. Wilcoxon Rank Sum\n",
    "13. One Way ANOVA\n",
    "14. Kruskal-Wallis H test\n",
    "15. Friedman test\n",
    "16. Chi-Square test\n",
    "17. Fisher's exact test\n",
    "18. McNemar test\n",
    "19. Pearson's Correlation\n",
    "20. Cross correlation\n",
    "21. Normalized Cross correlation\n",
    "22. Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypotheses\n",
    "### $ {{H_0}: {\\mu_x} = {\\mu_y}} $\n",
    "\n",
    "### $ {{H_1}: {\\overline x} \\neq {\\overline y}} $\n",
    "\n",
    "# Distribution\n",
    "Chebyshev distribution\n",
    "\n",
    "<img src=\"chebyshev.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "The Gaussian equation is:\n",
    "\n",
    "### $ f(x \\; | \\; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2} } \\; e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} } $\n",
    "\n",
    "$\\mu$ is the *mean*\n",
    "\n",
    "$\\sigma$ is the *standard deviation*\n",
    "\n",
    "$\\sigma^2$ is the *variance*\n",
    "\n",
    "\n",
    "# Variance\n",
    "### $ s^2 = {\\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N-1} } $\n",
    "\n",
    "# Standard deviation\n",
    "### $ s = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N-1} } $\n",
    "\n",
    "# Shapiro-Wilk\n",
    "The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.\n",
    "\n",
    "Rejecting or disproving the null hypothesis—and thus concluding that there are grounds for believing that there is a relationship between two phenomena (e.g. that a potential treatment has a measurable effect)—is a central task in the modern practice of science; the field of statistics gives precise criteria for rejecting a null hypothesis\n",
    "\n",
    "### $ W = {\\left(\\sum_{i=1}^n a_i x_{(i)}\\right)^2 \\over \\sum_{i=1}^n (x_i-\\overline{x})^2} $\n",
    "\n",
    "where\n",
    "\n",
    "$ {\\left(\\sum_{i=1}^n a_i x_{(i)}\\right)^2} $\n",
    "\n",
    "is the Q-Q (quantile plot squared). If it is a normal distribution the numerator should be the slope of the variance $ \\sigma^2 $\n",
    "\n",
    "Thus, the denominator is also a line of variance $ \\sigma^2 $, based upon the expected normal distribution of a sample matching the actual samples mean and variance.\n",
    "\n",
    "\n",
    "# Z-score\n",
    "Calculate the z score of each value in the sample, relative to the sample mean and standard deviation.\n",
    "### $ z = {x- \\mu \\over \\sigma} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"which_test.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"which_test.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired ttest\n",
    "### $ t = \\frac{\\sum D}{\\sqrt{\\frac{n\\sum D^2 - (\\sum D)^2}{N-1}} } $\n",
    "\n",
    "$ D $ is the Difference between paired values\n",
    "\n",
    "$ D^2 $ is the Difference squared between paired values\n",
    "\n",
    "$ \\alpha $ = 0.05\n",
    "\n",
    "$ dF = n -1 $\n",
    "\n",
    "When we recieve a value for $ t_{stat} $ we lookup a $ t_{crit} $ value in a t-distribution table. \n",
    "\n",
    "$ t_{stat} $ may be negative or positive\n",
    "\n",
    "We look for intersection in degrees of Freedom and the probability value $ {\\frac{0.05}{2} = 0.025} $ for a two-tailed test.\n",
    "\n",
    "<img src=\"tstat.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Test\n",
    "Perform prior to unpaired ttest to see whether the groups are equal or unequal variance\n",
    "\n",
    "It can be calculated by dividing the larger variance by the smaller variance.\n",
    "\n",
    "\n",
    "### $ F = \\frac{S_X^2}{S_Y^2} $\n",
    "\n",
    "Along with the degrees of freedom (dF) for each sample, a lookup in an F-distribution table for *Fcrit* tells us whether the samples vary similarly. \n",
    "\n",
    "If F < Fcrit then the variance is equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students ttest\n",
    "Equal or unequal sample sizes, equal variance\n",
    "\n",
    "This test is used only when it can be assumed that the two distributions have the same variance. (We thus, pool the variances) Note that the previous formulae are a special case valid when both samples have equal sizes: n = n1 = n2. The t statistic to test whether the means are different can be calculated as follows:\n",
    "\n",
    "# $ t = \\frac{\\bar {X}_1 - \\bar{X}_2}{\\sqrt{\\frac{Sp^2}{n_1}+\\frac{Sp^2}{n_2}}} $\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "# $ s_p^2 = {\\frac{(n_1-1)s_{X_1}^2+(n_2-1)s_{X_2}^2}{n_1+n_2-2}} $\n",
    "\n",
    "is an estimator of the pooled standard deviation of the two samples: it is defined in this way so that its square is an unbiased estimator of the common variance whether or not the population means are the same. In these formulae, ni − 1 is the number of degrees of freedom for each group, and the total sample size minus two (that is, n1 + n2 − 2) is the total number of degrees of freedom, which is used in significance testing.\n",
    "\n",
    "Yields a t-statistic. Use a t-table with $ \\alpha = 0.05 $ and dF.\n",
    "\n",
    "# $ df = n_1 + n_2 - 2 $\n",
    "\n",
    "If tstat > tcrit we reject null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welch's t-test\n",
    "is used only when the two population variances are not assumed to be equal (the two sample sizes may or may not be equal) and hence must be estimated separately. The t statistic to test whether the population means are different is calculated as:\n",
    "\n",
    "# $ t = \\frac{\\overline{X}_1 - \\overline{Y}_2}{\\sqrt({s_x^2 \\over n_1} + {s_y^2  \\over n_2})} $\n",
    "\n",
    "Here ${s_1^2}$ is the unbiased estimator of the variance of each of the two samples.\n",
    "\n",
    "# $ \\mathrm{d.f.} = \\frac{(s_x^2/n_1 + s_y^2/n_2)^2}{(s_x^2/n_1)^2/(n_1-1) + (s_y^2/n_2)^2/(n_2-1)} $\n",
    "\n",
    "This is known as the Welch–Satterthwaite equation. The true distribution of the test statistic actually depends (slightly) on the two unknown population variances (see Behrens–Fisher problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wilcoxon Signed Rank Test\n",
    "\n",
    "Null hypothesis is that the Medians are the same\n",
    "\n",
    "$ {{H_0}: {M_x} = {M_y}} $\n",
    "\n",
    "$ {{H_1}: {M_x} \\neq {M_y}} $\n",
    "\n",
    "$ W = \\sum_{i=1}^{N_r} [sgn(x_{2,i} - x_{1,i}) \\cdot R_i] $\n",
    "\n",
    "Group into T- and T+ and add all the ranks. The lowest total is $ W_{STAT} $\n",
    "\n",
    "We get $ W_{CRIT} $ from Wilcoxon Signed Rank Table of values\n",
    "\n",
    "If $ W_{STAT} < W_{CRIT} $ we reject the null hypothesis\n",
    "\n",
    "Normally we reject if STAT is greater than CRIT. Opposite in case of Wilcoxon Signed Rank Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney U test\n",
    "\n",
    "Unpaired non-normally distributed + equal variance\n",
    "\n",
    "Or ranked/ratings\n",
    "\n",
    "$ {H_0} $: No difference betwen ranks of treatments\n",
    "\n",
    "$ {H_1} $: There is a difference betwen ranks of treatments\n",
    "\n",
    "Lets assume we have two treatment groups x and y (unpaired). Each with a single value.\n",
    "\n",
    "##### Step 1\n",
    "We pool all values from condition x and y and rank them. \n",
    "\n",
    "##### Step 2\n",
    "We then associate each value in x and y with its rank from the pooled data.\n",
    "\n",
    "##### Step 3\n",
    "We calculate the individual $ U_{STAT} $ for group x and y.\n",
    "\n",
    "$ U_{STAT} = RANK SUM - \\frac{n(n+1)}{2} $\n",
    "\n",
    "##### Step 4\n",
    "We take the smallest $ U_{STAT} $ and compare to $ U_{STAT} $.\n",
    "\n",
    "$ U_{STAT} $ < $ U_{STAT} $ we reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wilcoxon Rank Sum Test\n",
    "\n",
    "unpaired, non-normally distributed, unequal variance\n",
    "\n",
    "$ {H_0} $: No difference betwen ranks of treatments\n",
    "\n",
    "$ {H_1} $: There is a difference betwen ranks of treatments\n",
    "\n",
    "Lets assume we have two treatment groups x and y (unpaired). Each with a single value.\n",
    "\n",
    "##### Step 1\n",
    "We pool all values from condition x and y and rank them. \n",
    "\n",
    "##### Step 2\n",
    "We then associate each value in x and y with its rank from the pooled data.\n",
    "\n",
    "##### Step 3\n",
    "We take the group with the smallest sample size, lets assume its group x.\n",
    "\n",
    "$ R $ = sum of all the ranks of group x\n",
    "\n",
    "##### Step 4\n",
    "We assume that if both x and y were taken from the same distribution then the rank sum of x should be equal to the expected Rank sum of the whole pouplation/2\n",
    "\n",
    "# $ \\mu_R = \\frac{n_1(n_1+n_2+1)}{2} $\n",
    "\n",
    "# $ \\sigma_R = \\sqrt \\frac{n_1 n_2(n_1+n_2+1)}{12} $\n",
    "\n",
    "##### Step 5\n",
    "\n",
    "Our test statistic is \n",
    "\n",
    "# $ Z = \\frac{R - U_{R}}{\\sigma_{R}} $\n",
    "\n",
    "where $ U_R $ and $ \\sigma_R $ are the expected mean and std of the ranks\n",
    "\n",
    "##### Step 6\n",
    "If $ Z $ is > +/- 1.96 its outside the 95 percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-way ANOVA (unpaired-equal variance)\n",
    "\n",
    "How do we find the variance in individual groups and between them... Then we will look at multiple groups.\n",
    "\n",
    "##### Sample Variance for a single group\n",
    "\n",
    "# $ s^2 = {\\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N-1} } $\n",
    "\n",
    "\n",
    "##### F-test for two groups\n",
    "\n",
    "# $ F = \\frac{S_X^2}{S_Y^2} $\n",
    "\n",
    "##### For multiple groups!\n",
    "\n",
    "# $ F = \\frac{MSS_B}{MSS_W} $\n",
    "\n",
    "where $ MSS_B = $ mean sum of squares between the groups and $ MSS_W = $ the mean sum of squares within the groups\n",
    "\n",
    "\n",
    "\n",
    "# $ MSS_W = \\frac {\\sum_{g \\in G} (X - \\overline X_g)^2} {n - k} $\n",
    "\n",
    "Thus, for each value $ g $ in a group, $ \\in G $ for all groups, we calculate each value $X$ minus the mean of each group $\\overline X_g$ and square. we then divide by the total number of variables $n$ and the total number of groups $k$.\n",
    "\n",
    "\n",
    "\n",
    "# $ MSS_B = \\frac {\\sum_{g \\in G} n_g(\\overline X_g - \\overline X_G)^2} {k - 1} $\n",
    "\n",
    "Thus, for each value $ g $ in a group, we multiply the number in each group$n_g$ by the mean of the group $X_g$ minus the mean of all goups $X_G$ divided by the total number of groups minus 1 $k - 1$\n",
    "\n",
    "Then we look up in an F table using:\n",
    "\n",
    "# $ df_W = n - k $\n",
    "\n",
    "# $ df_B = k - 1 $\n",
    "\n",
    "\n",
    "So, our hypotheses are...\n",
    "\n",
    "$ {{H_0}: {\\mu_1} = {\\mu_2} = {\\mu_3}} $\n",
    "\n",
    "$ {{H_1}: {\\mu_1} \\neq {\\mu_2} \\neq {\\mu_3}} $\n",
    "\n",
    "If $ F_{STAT} > F_{CRIT} $ we reject $ H_0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kruskal-Wallis H test (para-unpaired-unequal variance)\n",
    "\n",
    "non-para version of ANOVA, ordinal, ratings etc or not not normally distributed\n",
    "\n",
    "$ H_0 $: The three probability distributions are the same\n",
    "\n",
    "$ H_1 $: The three probability distributions are not the same\n",
    "\n",
    "# $ H = \\frac{12}{n(n+1)} \\sum \\frac{{R_i}^2}{n_i} - 3(n+1) $\n",
    "\n",
    "$n$ = total number values across all groups\n",
    "\n",
    "$R_i$ the Ranks for each numbers of groups\n",
    "\n",
    "$n_i$ the number of values in each group\n",
    "\n",
    "##### Step 1\n",
    "# $ H = \\frac{12}{n(n+1)}$\n",
    "\n",
    "##### Step 2\n",
    "# $ \\sum \\frac{{R_i}^2}{n_i} $\n",
    "Pool and rank all data. Sum the ranks in each group, square them, and divide by number of values in that group... add to next groups data.\n",
    "\n",
    "##### Step 3\n",
    "$ - 3(n+1) $\n",
    "\n",
    "##### Step 4\n",
    "This yields a $ H_{STAT} $.\n",
    "\n",
    "The $dF = n - 1$.\n",
    "\n",
    "We then use a $ ChiSquare_{CRIT} $\n",
    "\n",
    "If $ H_{STAT} > ChiSquare_{CRIT} $ we reject $ H_0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friedman (paired - non para)\n",
    "\n",
    "$ H_0 : M_1 = M_2 = ... = M_k $\n",
    "\n",
    "$ H_1 $: At least one equality is violated\n",
    "\n",
    "The procedure involves ranking each row (or block) together, then considering the values of ranks by columns.\n",
    "\n",
    "# $ X_r^2 = \\frac{12}{bk(k+1)} \\sum_{j=1}^K {{R_j}^2} - 3b(k+1) $\n",
    "\n",
    "<img src=\"fried.png\">\n",
    "\n",
    "With 3 groups and 9 blocks $ b =9, k = 3 $\n",
    "\n",
    "We the rank, in blocks, across the data based upon magnitude to yield the following.\n",
    "\n",
    "We also sum the columns.\n",
    "\n",
    "<img src=\"friedman.png\">\n",
    "\n",
    "We can now plug the figures into our equation\n",
    "\n",
    "# $ X_r^2 = \\frac{12}{bk(k+1)} \\sum_{j=1}^K {{R_j}^2} - 3b(k+1) $\n",
    "\n",
    "# $ X_r^2 = \\frac{12}{(9)(3)(4)}(19.5^2 + 9^2 + 25.5^2) - (3)(9)(4) $\n",
    "\n",
    "# $ = 15.5 $\n",
    "\n",
    "Our $ ChiSquare_{CRIT} $ has $ df : k - 1 = 2 $. This value for $ ChiSquare_{CRIT} $ is 5.991.\n",
    "\n",
    "As $ X_r^2 > ChiSquare_{CRIT} $ we reject $ H_0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Square (goodness of fit) Test\n",
    "\n",
    "Without other qualification, 'chi-squared test' often is used as short for Pearson's chi-squared test. The chi-squared test is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories.\n",
    "\n",
    "Chi-squared tests are often constructed from a sum of squared errors, or through the sample variance. Test statistics that follow a chi-squared distribution arise from an assumption of independent normally distributed data, which is valid in many cases due to the central limit theorem. A chi-squared test can be used to attempt rejection of the null hypothesis that the data are independent.\n",
    "\n",
    "Poker machine that deals cards\n",
    "\n",
    "Hearts 441, Spades 404, Diamonds 402, Clubs 353, Sum (1600)\n",
    "\n",
    "Could it be that these suits are equally likely or are these discrepancies too much to be random?\n",
    "\n",
    "# $ \\chi^2 = \\sum \\frac {(O - E)^2}{E} $\n",
    "\n",
    "$ O $ Observed values\n",
    "\n",
    "$ E $ Expected values\n",
    "\n",
    "$ H_0: P_{Hearts} = P_{Spades} = P_{Diamonds} = P_{Clubs} $\n",
    "\n",
    "$ H_1: P_{Hearts} \\neq P_{Spades} \\neq P_{Diamonds} \\neq P_{Clubs} $\n",
    "\n",
    "$ df = n.  of groups - 1 = 3 $\n",
    "\n",
    "$ ChiSquare_{STAT} $ = 9.77\n",
    "\n",
    "$ ChiSquare_{CRIT} $ = 7.81\n",
    "\n",
    "<img src=\"chi.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fishers Exact test\n",
    "\n",
    "For example, a sample of teenagers might be divided into male and female on the one hand, and those that are and are not currently studying for a statistics exam on the other. We hypothesize, for example, that the proportion of studying individuals is higher among the women than among the men, and we want to test whether any difference of proportions that we observe is significant. The data might look like this:\n",
    "\n",
    "<img src=\"fish1.png\">\n",
    "\n",
    "\n",
    "The question we ask about these data is: knowing that 10 of these 24 teenagers are studiers, and that 12 of the 24 are female, and assuming the null hypothesis that men and women are equally likely to study, what is the probability that these 10 studiers would be so unevenly distributed between the women and the men? If we were to choose 10 of the teenagers at random, what is the probability that 9 or more of them would be among the 12 women, and only 1 or fewer from among the 12 men?\n",
    "\n",
    "Before we proceed with the Fisher test, we first introduce some notations. We represent the cells by the letters a, b, c and d, call the totals across rows and columns marginal totals, and represent the grand total by n. So the table now looks like this:\n",
    "\n",
    "<img src=\"fish2.png\">\n",
    "\n",
    "Fisher showed that the probability of obtaining any such set of values was given by the hypergeometric distribution:\n",
    "\n",
    "$ p = \\frac{ \\displaystyle{{a+b}\\choose{a}} \\displaystyle{{c+d}\\choose{c}} }{ \\displaystyle{{n}\\choose{a+c}} } = \\frac{(a+b)!~(c+d)!~(a+c)!~(b+d)!}{a!~~b!~~c!~~d!~~n!} $\n",
    "\n",
    "$ p = { {\\tbinom{10}{1}} {\\tbinom{14}{11}} }/{ {\\tbinom{24}{12}} } = \\tfrac{10!~14!~12!~12!}{1!~9!~11!~3!~24!} $\n",
    "\n",
    "$ = 0.001346076 $\n",
    "\n",
    "The formula above gives the exact hypergeometric probability of observing this particular arrangement of the data, assuming the given marginal totals, on the null hypothesis that men and women are equally likely to be studiers. To put it another way, if we assume that the probability that a man is a studier is $p$, the probability that a woman is a studier is also $p$, and we assume that both men and women enter our sample independently of whether or not they are studiers, then this hypergeometric formula gives the conditional probability of observing the values a, b, c, d in the four cells, conditionally on the observed marginals (i.e., assuming the row and column totals shown in the margins of the table are given). This remains true even if men enter our sample with different probabilities than women. The requirement is merely that the two classification characteristics—gender, and studier (or not)—are not associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McNemar test\n",
    "\n",
    "In statistics, McNemar's test is a statistical test used on paired nominal data. It is applied to 2 × 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is \"marginal homogeneity\")\n",
    "\n",
    "The test is applied to a 2 × 2 contingency table, which tabulates the outcomes of two tests on a sample of n subjects, as follows.\n",
    "\n",
    "<img src=\"mc1.png\">\n",
    "\n",
    "$a and d$ are concodant pairs.\n",
    "\n",
    "$b anc c$ are the concordant pairs.\n",
    "\n",
    "Thus the null and alternative hypotheses are:\n",
    "\n",
    "$ H_0: P_b = P_c $\n",
    "\n",
    "$ H_1: P_b \\neq P_c $\n",
    "\n",
    "Here pa, etc., denote the theoretical probability of occurrences in cells with the corresponding label.\n",
    "\n",
    "The McNemar test statistic is:\n",
    "\n",
    "# $ \\chi^2 = {(b-c)^2 \\over b+c} $\n",
    "\n",
    "Under the null hypothesis, with a sufficiently large number of discordants (cells b and c), $\\chi^{2}$ has a chi-squared distribution with 1 degree of freedom. If the $\\chi^{2}$ result is significant, this provides sufficient evidence to reject the null hypothesis, in favour of the alternative hypothesis that pb ≠ pc, which would mean that the marginal proportions are significantly different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearsons correlation coefficient\n",
    "\n",
    "is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation\n",
    "\n",
    "# $ \\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y} $\n",
    "\n",
    "expanded to...\n",
    "\n",
    "# $ r =\\frac{\\sum ^n _{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum ^n _{i=1}(x_i - \\bar{x})^2} \\sqrt{\\sum ^n _{i=1}(y_i - \\bar{y})^2}} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation\n",
    "\n",
    "-1 to 1\n",
    "\n",
    "# $ corr(x, y) = \\sum_{n=0}^{n-1} x[n]*y[n] $\n",
    "\n",
    "# Normalized Cross-Correlation\n",
    "\n",
    "The normalized cross-correlation can detect the correlation of two signals with different power.\n",
    "\n",
    "It takes the cross correlation equation and scales the function by the energy of the two signals in the denominator.\n",
    "\n",
    "# $ norm\\_corr(x,y)=\\dfrac{\\sum_{n=0}^{n-1} x[n]*y[n]}{\\sqrt{\\sum_{n=0}^{n-1} x[n]^2 * \\sum_{n=0}^{n-1} y[n]^2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
